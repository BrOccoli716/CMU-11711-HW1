{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23dec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(nums, i):\n",
    "\tif i == -1 or i == len(nums):\n",
    "\t\treturn -float('inf')\n",
    "\telse:\n",
    "\t\treturn nums[i]\n",
    "def find_top(nums):\n",
    "    if len(nums) == 0:\n",
    "        return -1\n",
    "    if len(nums) == 1:\n",
    "        return nums[0]\n",
    "    l, r = 0, len(nums) - 1\n",
    "    while l <= r:\n",
    "        mid = (l + r) // 2  # mid = l + (r â€“ l) // 2\n",
    "        if get(nums, mid - 1) < get(nums, mid) > get(nums, mid + 1):\n",
    "            return mid\n",
    "        if get(nums, mid - 1) > get(nums, mid):\n",
    "            r = mid - 1\n",
    "        elif get(nums, mid + 1) > get(nums, mid):\n",
    "            l = mid + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f9a2323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = [3, 1, 2, 4]\n",
    "find_top(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92c23714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: {'train': 'data/cfimdb-train.txt', 'dev': 'data/cfimdb-dev.txt', 'test': 'data/cfimdb-test.txt', 'label_names': 'data/cfimdb-label-mapping.json', 'pretrained_model_path': 'stories42M.pt', 'max_sentence_len': None, 'seed': 1337, 'epochs': 5, 'option': 'generate', 'use_gpu': False, 'generated_sentence_low_temp_out': 'generated-sentence-temp-0.txt', 'generated_sentence_high_temp_out': 'generated-sentence-temp-1.txt', 'dev_out': 'cfimdb-dev-prompting-output.txt', 'test_out': 'cfimdb-test-prompting-output.txt', 'lora_rank': 4, 'lora_alpha': 1.0, 'batch_size': 8, 'hidden_dropout_prob': 0.3, 'lr': 2e-05}\n",
      "load 1707 data from data/cfimdb-train.txt\n",
      "tensor([192, 177, 237, 195, 390, 227, 172, 250])\n"
     ]
    }
   ],
   "source": [
    "from run_llama import LlamaDataset, create_data\n",
    "import argparse\n",
    "from tokenizer import Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_args():\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\tparser.add_argument(\"--train\", type=str, default=\"data/cfimdb-train.txt\")\n",
    "\tparser.add_argument(\"--dev\", type=str, default=\"data/cfimdb-dev.txt\")\n",
    "\tparser.add_argument(\"--test\", type=str, default=\"data/cfimdb-test.txt\")\n",
    "\tparser.add_argument(\"--label-names\", type=str, default=\"data/cfimdb-label-mapping.json\")\n",
    "\tparser.add_argument(\"--pretrained-model-path\", type=str, default=\"stories42M.pt\")\n",
    "\tparser.add_argument(\"--max_sentence_len\", type=int, default=None)\n",
    "\tparser.add_argument(\"--seed\", type=int, default=1337)\n",
    "\tparser.add_argument(\"--epochs\", type=int, default=5)\n",
    "\tparser.add_argument(\"--option\", type=str,\n",
    "\t\t\t\t\t\thelp='prompt: the Llama parameters are frozen; finetune: Llama parameters are updated',\n",
    "\t\t\t\t\t\tchoices=('generate', 'prompt', 'finetune', 'lora'), default=\"generate\")\n",
    "\tparser.add_argument(\"--use_gpu\", action='store_true')\n",
    "\tparser.add_argument(\"--generated_sentence_low_temp_out\", type=str, default=\"generated-sentence-temp-0.txt\")\n",
    "\tparser.add_argument(\"--generated_sentence_high_temp_out\", type=str, default=\"generated-sentence-temp-1.txt\")\n",
    "\tparser.add_argument(\"--dev_out\", type=str, default=\"cfimdb-dev-prompting-output.txt\")\n",
    "\tparser.add_argument(\"--test_out\", type=str, default=\"cfimdb-test-prompting-output.txt\")\n",
    "\tparser.add_argument(\"--lora_rank\", type=int, default=4, help=\"LoRA rank (r)\")\n",
    "\tparser.add_argument(\"--lora_alpha\", type=float, default=1.0, help=\"LoRA alpha scaling factor\")\n",
    "\n",
    "\t# hyper parameters\n",
    "\tparser.add_argument(\"--batch_size\", help='sst: 64, cfimdb: 8 can fit a 12GB GPU', type=int, default=8)\n",
    "\tparser.add_argument(\"--hidden_dropout_prob\", type=float, default=0.3)\n",
    "\tparser.add_argument(\"--lr\", type=float, help=\"learning rate, default lr for 'pretrain': 1e-3, 'finetune': 1e-5\",\n",
    "\t\t\t\t\t\tdefault=2e-5)\n",
    "\n",
    "\targs, _ = parser.parse_known_args()\n",
    "\tprint(f\"args: {vars(args)}\")\n",
    "\treturn args\n",
    "args = get_args()\n",
    "\n",
    "tokenizer = Tokenizer(args.max_sentence_len)\n",
    "train_data, num_labels = create_data(args.train, tokenizer, 'train')\n",
    "train_dataset = LlamaDataset(train_data, args)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=args.batch_size,\n",
    "\t\t\t\t\t\t\t\t  collate_fn=train_dataset.collate_fn)\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    b_ids, b_labels, b_sents, b_padding_mask = batch['token_ids'], batch['labels'], batch['sents'], batch['padding_mask']\n",
    "    print(b_padding_mask.sum(dim=1))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
